{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1v22F3bK3mWX",
        "outputId": "21f288f4-e573-4d24-e5b0-bf9e85803138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-generativeai<0.9.0,>=0.8.0 (from langchain_google_genai)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting langchain-core<0.4,>=0.3.0 (from langchain_google_genai)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.9.2)\n",
            "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.24.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading langsmith-0.1.135-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (24.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.23.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.2.2)\n",
            "Downloading langchain_google_genai-2.0.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.135-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, requests-toolbelt, jsonpatch, httpcore, httpx, langsmith, langchain-core, google-ai-generativelanguage, google-generativeai, langchain_google_genai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.6\n",
            "    Uninstalling google-ai-generativelanguage-0.6.6:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.6\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.7.2\n",
            "    Uninstalling google-generativeai-0.7.2:\n",
            "      Successfully uninstalled google-generativeai-0.7.2\n",
            "Successfully installed google-ai-generativelanguage-0.6.10 google-generativeai-0.8.3 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.10 langchain_google_genai-2.0.1 langsmith-0.1.135 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "db649083aa50457bb3b53d199dceb343"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.3 (from langchain_community)\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.135)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.3->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-text-splitters, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.3.3 langchain-text-splitters-0.3.0 langchain_community-0.3.2 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.135)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.24.7)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.3.10)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.1.135)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.1.0 sentence-transformers-3.2.0\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (0.1.135)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.11.0 langchain_groq-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_google_genai\n",
        "!pip install langchain_community\n",
        "!pip install langchain_community\n",
        "!pip install langchain_huggingface\n",
        "!pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n"
      ],
      "metadata": {
        "id": "3BcM9n9t5a5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
      ],
      "metadata": {
        "id": "nERYJxFL5yL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Chain (Legacy chaining or old chaining concept)**"
      ],
      "metadata": {
        "id": "Ukn05ssg8LtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n\""
      ],
      "metadata": {
        "id": "r47G_uf_6met"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"skill\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "TqQBECV97Cy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P2wOOkJ7NIY",
        "outputId": "3697e228-bac8-4f0f-9573-2cd83750d8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['skill'] input_types={} partial_variables={} template='Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain"
      ],
      "metadata": {
        "id": "Fiq-lUco7TNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SUAt8Kh7a0S",
        "outputId": "ca389882-3ade-4393-a26d-8ceb8fac713b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5e4e4c67dc96>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(\"Generative AI\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFx7pf0Q7d8i",
        "outputId": "03998954-0753-4f70-8364-07cfb4256232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b0fae29bed8e>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(llm_chain.run(\"Generative AI\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's great! Generative AI is a fascinating and rapidly growing field. Here are the top 5 things you should learn, tailored for someone just starting out:\n",
            "\n",
            "1. **Foundational Machine Learning Concepts:**\n",
            "    * **Supervised vs. Unsupervised Learning:** Understand the difference between training models on labeled data (supervised) and letting them find patterns in unlabeled data (unsupervised, where generative models shine).\n",
            "    * **Basic Neural Networks:** Get familiar with the structure of a neural network, including layers, neurons, activation functions, and backpropagation (how they learn). Resources like Khan Academy, 3Blue1Brown (YouTube), and the Machine Learning Crash Course by Google are great starting points.\n",
            "2. **Key Generative Model Architectures:**\n",
            "    * **Generative Adversarial Networks (GANs):** Learn the fundamental idea behind GANs, with their generator and discriminator networks competing to create realistic data. \n",
            "    * **Variational Autoencoders (VAEs):** Understand how VAEs encode data into a latent space and then generate new data from that space.\n",
            "    * **Diffusion Models:** These are newer and very powerful. Learn how they gradually corrupt data with noise and then train a network to reverse the process, generating new samples.\n",
            "3. **Essential Tools and Libraries:**\n",
            "    * **Programming Language (Python):**  Python is the dominant language for AI and machine learning.\n",
            "    * **TensorFlow or PyTorch:** These deep learning libraries provide the building blocks for creating and training generative models. Choose one to focus on initially.\n",
            "    * **Datasets and Pre-trained Models:** Explore resources like Hugging Face, Kaggle Datasets, and Google Dataset Search for data to train your models or experiment with pre-trained ones.\n",
            "4. **Applications of Generative AI:**\n",
            "    * **Image Generation and Manipulation:**  Explore how generative models create realistic images, modify existing ones, and generate art.\n",
            "    * **Text Generation:** Learn how models like GPT-3 are used for writing stories, poems, code, and more.\n",
            "    * **Music and Audio Generation:** Discover how AI is composing music, creating sound effects, and even generating realistic speech.\n",
            "5. **Ethical Considerations:**\n",
            "    * **Bias and Fairness:** Generative models can inherit biases from their training data. Learn about techniques to mitigate bias.\n",
            "    * **Misinformation and Deepfakes:** Understand the potential misuse of generative AI for creating misleading content and the importance of responsible development.\n",
            "\n",
            "**Learning Resources:**\n",
            "\n",
            "* **Online Courses:** Coursera, edX, Udacity, and DeepAI offer excellent courses on generative AI.\n",
            "* **Books:** \"Generative Deep Learning\" by David Foster is a good starting point.\n",
            "* **Blogs and Tutorials:** Towards Data Science, Machine Learning Mastery, and Distill.pub provide valuable insights and practical guides.\n",
            "\n",
            "**Remember:** Start with the basics, be patient with yourself, and most importantly, have fun exploring the creative world of Generative AI! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run({'skill':\"Data Science\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Beif3CM7jvB",
        "outputId": "82b9fcee-bd6d-426a-8241-bfd2e8f5e525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's great you're diving into data science! It's a broad field, so here are 5 fundamental areas to focus on, along with why they're important:\n",
            "\n",
            "**1. Programming (Python or R):**\n",
            "\n",
            "* **Why:**  These languages are the workhorses of data science. You'll use them to clean, analyze, and visualize data, as well as build and implement models.\n",
            "* **How:** Start with one language (Python is generally more versatile). Take online courses, work through tutorials, and build small projects.\n",
            "\n",
            "**2. Math Fundamentals (Statistics, Linear Algebra, Calculus):**\n",
            "\n",
            "* **Why:** Data science is built on mathematical principles. Understanding these concepts helps you interpret data, choose the right models, and evaluate their performance.\n",
            "* **How:** Don't get bogged down in rigorous proofs. Focus on practical applications and intuition. Khan Academy, Coursera, and edX offer excellent courses.\n",
            "\n",
            "**3. Data Wrangling and Cleaning:**\n",
            "\n",
            "* **Why:**  Real-world data is messy! You'll spend a significant amount of time cleaning, transforming, and preparing data for analysis and modeling.\n",
            "* **How:** Learn techniques for handling missing values, dealing with different data types, and merging datasets. Libraries like Pandas (Python) and dplyr (R) are essential.\n",
            "\n",
            "**4. Machine Learning Algorithms:**\n",
            "\n",
            "* **Why:** Machine learning is the heart of many data science applications. You'll use algorithms to build predictive models, uncover patterns, and make data-driven decisions.\n",
            "* **How:** Start with the basics: linear regression, logistic regression, decision trees. Gradually explore more complex algorithms like support vector machines and neural networks.\n",
            "\n",
            "**5. Data Visualization and Communication:**\n",
            "\n",
            "* **Why:**  It's not enough to just analyze data; you need to communicate your findings effectively to both technical and non-technical audiences.\n",
            "* **How:** Learn to create clear, insightful visualizations using libraries like Matplotlib, Seaborn (Python), or ggplot2 (R). Practice explaining complex concepts in a simple and engaging way. \n",
            "\n",
            "**Remember:** Data science is an iterative and constantly evolving field. Be patient, keep learning, and don't be afraid to get your hands dirty with real-world projects! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Now using LCEL chaining concept**"
      ],
      "metadata": {
        "id": "dnoMdCwO8fl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCP_oL3l7-oh",
        "outputId": "e08b69e1-0aba-4dee-b68e-716e4a5521b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x79a418208c40>, default_metadata=())"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVEUxdMD8ngo",
        "outputId": "02a3c75f-4463-46f5-da4f-185110fbcc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['skill'], input_types={}, partial_variables={}, template='Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "p-jTUkI68qAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({'skill':\"Big Data\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ihuXcNn86gW",
        "outputId": "5173e8e5-4c37-4f30-b5c7-6dea41dc1a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"That's great! Big data is a fascinating and rapidly growing field. Here are 5 key areas to focus on as you begin your learning journey:\\n\\n**1. Programming Fundamentals:**\\n\\n* **Why:** Big data analysis requires you to work with massive datasets, and programming languages are essential for manipulating and extracting insights from them.\\n* **What to Learn:**\\n    * **Python:**  A versatile language with powerful libraries like Pandas (data manipulation) and Scikit-learn (machine learning).\\n    * **Java:** Used in big data frameworks like Hadoop and Spark. Strong for building scalable applications.\\n    * **Scala:**  A concise language that runs on the Java Virtual Machine (JVM) and is well-suited for Spark.\\n\\n**2. Distributed Computing Frameworks:**\\n\\n* **Why:**  Traditional data processing techniques struggle with massive datasets. Distributed computing frameworks break down tasks across multiple machines for faster analysis.\\n* **What to Learn:**\\n    * **Hadoop:** A foundational framework for distributed storage (HDFS) and processing (MapReduce) of large datasets.\\n    * **Spark:**  Faster than Hadoop for iterative processing, making it ideal for machine learning and real-time analytics.\\n\\n**3. Data Storage and Management:**\\n\\n* **Why:** Understanding how to store, organize, and retrieve large datasets efficiently is crucial for big data analysis.\\n* **What to Learn:**\\n    * **Relational Databases (SQL):**  Still relevant for structured data, but consider their limitations with massive datasets.\\n    * **NoSQL Databases:**  Offer flexibility and scalability for handling diverse data types (e.g., document, key-value, graph). Examples include MongoDB, Cassandra, and Neo4j.\\n    * **Cloud Storage:** Services like AWS S3, Google Cloud Storage, and Azure Blob Storage provide scalable and cost-effective options.\\n\\n**4. Data Visualization and Communication:**\\n\\n* **Why:**  Turning raw data into meaningful visualizations is key for understanding trends and communicating insights to others.\\n* **What to Learn:**\\n    * **Data Visualization Tools:**  Tableau, Power BI, and libraries like Matplotlib (Python) and D3.js (JavaScript) offer powerful ways to create charts, graphs, and dashboards.\\n    * **Storytelling with Data:**  Develop your ability to present data-driven narratives that effectively communicate findings and drive decision-making. \\n\\n**5. Big Data Analytics and Machine Learning:**\\n\\n* **Why:**  The ultimate goal of big data is to extract valuable insights. Analytics and machine learning techniques help you discover patterns, make predictions, and gain a deeper understanding of your data.\\n* **What to Learn:**\\n    * **Descriptive Analytics:**  Summarizing and describing historical data (e.g., averages, trends).\\n    * **Predictive Analytics:**  Using statistical models and machine learning to forecast future outcomes.\\n    * **Machine Learning Algorithms:**  Explore algorithms like linear regression, decision trees, and clustering for various analytical tasks.\\n\\n**Learning Resources:**\\n\\n* **Online Courses:** Coursera, edX, Udemy, DataCamp, and cloud provider platforms (AWS, Azure, Google Cloud) offer excellent big data courses.\\n* **Books:** Search for titles on Hadoop, Spark, NoSQL databases, and specific programming languages.\\n* **Open-Source Projects:** Contribute to projects on GitHub to gain practical experience.\\n* **Community:**  Join online forums, attend meetups, and connect with other learners and professionals in the field. \\n\\n**Remember:** Big data is a vast field. Start with a solid foundation in programming and distributed computing, then gradually expand your knowledge to other areas based on your interests and career goals. Good luck with your learning journey! \\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-4e9b227f-dddd-429d-ab46-bbdd5ec172ab-0' usage_metadata={'input_tokens': 21, 'output_tokens': 772, 'total_tokens': 793}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "W2pU9UMB8_8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "UvIPimRI9Vtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "rUh7rMYZ9aET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({'skill':\"DSA\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_HAxnVz9ia0",
        "outputId": "8c82f45f-badf-4280-c0ac-61aecd2b677a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's great! DSA (Data Structures and Algorithms) is crucial for any aspiring programmer. Here are the top 5 things to prioritize, focusing on practicality and a strong foundation:\n",
            "\n",
            "1. **Fundamental Data Structures:**\n",
            "   - **Arrays:** The building block; master operations like traversal, insertion, deletion, and searching.\n",
            "   - **Linked Lists:** Understand single, double, and circular linked lists.  Learn how they handle dynamic memory and their advantages/disadvantages compared to arrays.\n",
            "   - **Stacks and Queues:**  Grasp their LIFO (Last-In, First-Out) and FIFO (First-In, First-Out) behavior.  Explore real-world applications like function call stacks and task scheduling.\n",
            "   - **Trees:** Focus on binary trees, especially Binary Search Trees (BSTs). Learn about tree traversals (inorder, preorder, postorder) and how BSTs enable efficient searching and sorting.\n",
            "   - **Hash Tables:** Understand how they use hashing functions for near-constant-time average lookups.  Learn about collision handling techniques like chaining and open addressing.\n",
            "\n",
            "2. **Essential Algorithms:**\n",
            "   - **Sorting Algorithms:** Master at least two:\n",
            "     - **O(n log n) Efficiency:**  Merge Sort or Quick Sort (understand their trade-offs)\n",
            "     - **O(n^2) but with advantages:** Insertion Sort (efficient for nearly sorted data) or Selection Sort (for limited memory).\n",
            "   - **Searching Algorithms:**\n",
            "     - **Linear Search:** Simple but often sufficient.\n",
            "     - **Binary Search:** Very efficient for sorted data; understand its logarithmic time complexity.\n",
            "   - **Recursion:** A powerful problem-solving technique. Practice writing recursive functions and understanding the call stack.\n",
            "\n",
            "3. **Big O Notation:**\n",
            "   - Don't just memorize it — develop an intuitive understanding of time and space complexity.\n",
            "   - Be able to analyze the efficiency of your code and compare different algorithms.\n",
            "   - This is crucial for writing scalable code that performs well with large datasets.\n",
            "\n",
            "4. **Problem Solving:**\n",
            "   - **Platforms:** Choose a platform like LeetCode, HackerRank, or Codeforces.\n",
            "   - **Start Easy:** Begin with easier problems to build confidence.\n",
            "   - **Focus on Patterns:**  As you solve more, you'll recognize common patterns and techniques.\n",
            "   - **Don't Give Up:**  Problem-solving is challenging but rewarding. Embrace the process!\n",
            "\n",
            "5. **Implementation:**\n",
            "   - **Choose a Language:** Stick to one language initially (Python is beginner-friendly, C++ is common for competitive programming).\n",
            "   - **Write Clean Code:** Practice good coding style and make your code readable.\n",
            "   - **Test Thoroughly:**  Write test cases to ensure your solutions are correct.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "* **Start with a good resource:**  Find a well-structured course or book that suits your learning style.\n",
            "* **Practice consistently:**  Even short, regular practice sessions are more effective than infrequent cramming.\n",
            "* **Don't be afraid to ask for help:** Online communities and forums can be great resources.\n",
            "* **Have fun!**  DSA can be challenging but also incredibly rewarding. Enjoy the learning journey! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Runnables**"
      ],
      "metadata": {
        "id": "oAfchxLu9yzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Runnable is class in langchain which handles everything related to LCEL. LangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.**"
      ],
      "metadata": {
        "id": "IoPS33Jc95xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RunnablePassthrough` is a type of runnable in LangChain that simply passes the input it receives directly to the output without any transformation or processing. It acts as a \"pass-through\" operator, meaning it doesn't alter the data in any way, but is useful for situations where you want to chain multiple runnables and need a placeholder or debugging step.\n",
        "\n",
        "### Use Cases for `RunnablePassthrough`:\n",
        "\n",
        "1. **Debugging**: If you want to inspect or log the input at a specific stage in a chain of runnables, you can insert a `RunnablePassthrough` to capture and examine the data without modifying it.\n",
        "  \n",
        "2. **Conditional Operations**: In certain workflows, you might want to bypass certain operations under certain conditions, and `RunnablePassthrough` can serve as a placeholder for such situations, allowing for flexible and dynamic pipeline configurations.\n",
        "\n",
        "3. **Maintaining Chain Structure**: When constructing complex workflows with optional or conditional steps, `RunnablePassthrough` can be used to maintain the chain structure, allowing you to skip over certain steps while keeping the overall flow intact.\n",
        "\n",
        "### Example of `RunnablePassthrough` in Action:\n",
        "\n",
        "```python\n",
        "from langchain.runnables import RunnablePassthrough\n",
        "\n",
        "# Assume we have some other runnables\n",
        "lowercase_runnable = LowerCaseRunnable()  # Converts text to lowercase\n",
        "model_runnable = LanguageModelRunnable()  # Calls a language model\n",
        "\n",
        "# Create a runnable pass-through\n",
        "passthrough_runnable = RunnablePassthrough()\n",
        "\n",
        "# Chain them together\n",
        "sequence = lowercase_runnable.pipe(passthrough_runnable).pipe(model_runnable)\n",
        "\n",
        "# Invoke the sequence\n",
        "result = sequence.invoke(\"HELLO WORLD\")\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The `lowercase_runnable` converts the input text \"HELLO WORLD\" to \"hello world\".\n",
        "- The `RunnablePassthrough` simply passes the lowercase text to the next runnable.\n",
        "- The `model_runnable` takes the lowercase text and processes it (e.g., by sending it to a language model).\n",
        "\n",
        "### Benefits:\n",
        "- **No-Op Behavior**: It provides a \"no-operation\" behavior, meaning it doesn't modify data, but can be part of a runnable chain.\n",
        "- **Flexibility**: Useful when you need a placeholder runnable in more complex pipelines.\n",
        "- **Transparency**: Ensures that certain parts of a chain remain transparent, where no processing is required.\n",
        "\n",
        "In essence, `RunnablePassthrough` is a simple but powerful utility in LangChain to help manage workflows and pipelines without altering data."
      ],
      "metadata": {
        "id": "Q796_m9agvPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda"
      ],
      "metadata": {
        "id": "FQ0BaEua9l8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnablePassthrough() # Now the interpreter should find RunnablePassthrough"
      ],
      "metadata": {
        "id": "X6E_iBTqdq8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Hello, myself Ayush...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4yuO8MIpfq6d",
        "outputId": "4005688a-03ce-49c2-8dc7-5f99833e26bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, myself Ayush...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnablePassthrough() | RunnablePassthrough() |RunnablePassthrough()"
      ],
      "metadata": {
        "id": "_rLbMq2GgKNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Hello, myself Ayush Shaurya Jha...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NUm3IaZkgc62",
        "outputId": "073daa1a-47eb-48e8-dce2-1a7fe52232ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, myself Ayush Shaurya Jha...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Runnable Lambda**\n",
        "**RunnableLambda** in LangChain is a flexible and powerful type of runnable that allows you to define custom behavior by providing a lambda function (or any callable). It essentially acts as a wrapper around any Python function, allowing you to incorporate custom logic into LangChain's runnable pipelines."
      ],
      "metadata": {
        "id": "KpEuuZTdhCUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_upper(input):\n",
        "  return input.upper()"
      ],
      "metadata": {
        "id": "s8cc_GFmhBRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnablePassthrough() | RunnableLambda(string_upper)"
      ],
      "metadata": {
        "id": "PoZ_D4ykghJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Hello, myself Ayush Shaurya Jha...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Frg9j8SwhniK",
        "outputId": "609d0360-24f5-472d-8585-af8e9804f696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HELLO, MYSELF AYUSH SHAURYA JHA...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_upper.invoke(\"Hello, myself Ayush Shaurya Jha...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "2unTOzwkhqwM",
        "outputId": "3a55dbcb-7212-4366-eae0-a3d75e486924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'invoke'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-36268424c359>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstring_upper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, myself Ayush Shaurya Jha...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'invoke'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that function \"String_upper\" doesn't has invoke method but still intacted in chain because RunnableLambda has access to invoke method. Each component of chain must have invoke method."
      ],
      "metadata": {
        "id": "NP5Ce1Kth85G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Medium Document : Must READ**](https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac#:~:text=James%20Li%20%7C%20Medium-,Mental%20framework%20to%20building%20chains%20with%20LangChain%20Expression%20Language%20(LCEL,and%20merging%20chains%20as%20examples&text=LCEL%20(LangChain%20Expression%20Language)%20simplifies,confusing%20once%20we%20add%20complexity.)"
      ],
      "metadata": {
        "id": "M1Z1cmKeneR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RunnableParallel**\n",
        "`RunnableParallel` in LangChain is a runnable that allows you to execute multiple runnables concurrently (in parallel) and return their results together. It is particularly useful when you need to run multiple tasks independently and want to collect their results without waiting for each one to finish before starting the next. This can lead to more efficient execution, especially in workflows where some tasks might take longer than others.\n",
        "\n",
        "### How `RunnableParallel` Works:\n",
        "\n",
        "- **Concurrent Execution**: Multiple runnables are executed in parallel, meaning they don’t block each other.\n",
        "- **Result Aggregation**: Once all the runnables finish execution, their results are combined and returned as a collection (e.g., a list or dictionary).\n",
        "- **Efficiency**: Instead of processing runnables sequentially, `RunnableParallel` reduces wait time by running them concurrently, which can lead to faster results when dealing with I/O-bound or long-running operations.\n",
        "\n",
        "### Example of `RunnableParallel`:\n",
        "\n",
        "Let’s say you want to run two independent chains in parallel, like querying two different APIs or running two different machine learning models, and you want to combine their results afterward.\n",
        "\n",
        "```python\n",
        "from langchain.runnables import RunnableParallel, RunnableLambda\n",
        "\n",
        "# Define two example runnables\n",
        "runnable1 = RunnableLambda(lambda x: f\"Processed by runnable 1: {x}\")\n",
        "runnable2 = RunnableLambda(lambda x: f\"Processed by runnable 2: {x}\")\n",
        "\n",
        "# Combine them into a RunnableParallel\n",
        "parallel_runnable = RunnableParallel([runnable1, runnable2])\n",
        "\n",
        "# Invoke with input\n",
        "result = parallel_runnable.invoke(\"Hello, world!\")\n",
        "print(result)\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```python\n",
        "[\n",
        "    \"Processed by runnable 1: Hello, world!\",\n",
        "    \"Processed by runnable 2: Hello, world!\"\n",
        "]\n",
        "```\n",
        "\n",
        "In this case, the input `\"Hello, world!\"` is passed to both `runnable1` and `runnable2` at the same time, and their results are returned as a list.\n",
        "\n",
        "### Example with Different Types of Runnables:\n",
        "\n",
        "```python\n",
        "# Suppose you have two independent tasks\n",
        "def fetch_data_from_api(query):\n",
        "    # Simulate fetching data from an API\n",
        "    return f\"Data from API for {query}\"\n",
        "\n",
        "def process_data(query):\n",
        "    # Simulate data processing\n",
        "    return f\"Processed data for {query}\"\n",
        "\n",
        "# Wrap them in RunnableLambda\n",
        "api_runnable = RunnableLambda(lambda x: fetch_data_from_api(x))\n",
        "process_runnable = RunnableLambda(lambda x: process_data(x))\n",
        "\n",
        "# Create a parallel runnable\n",
        "parallel_runnable = RunnableParallel([api_runnable, process_runnable])\n",
        "\n",
        "# Invoke them with input\n",
        "result = parallel_runnable.invoke(\"example query\")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```python\n",
        "[\n",
        "    \"Data from API for example query\",\n",
        "    \"Processed data for example query\"\n",
        "]\n",
        "```\n",
        "\n",
        "### Benefits of `RunnableParallel`:\n",
        "\n",
        "1. **Improved Efficiency**: Tasks that can run concurrently (e.g., I/O-bound tasks like API calls or independent data processing) don’t block each other, leading to faster overall execution.\n",
        "   \n",
        "2. **Scalability**: In scenarios where many independent tasks need to be performed, `RunnableParallel` can help scale the workflow by handling multiple tasks simultaneously.\n",
        "\n",
        "3. **Unified Result**: The results of each runnable are aggregated together, typically in a list or dictionary format, making it easy to handle the output in the next step of the workflow.\n",
        "\n",
        "### Example Using a Dictionary with `RunnableParallel`:\n",
        "\n",
        "You can also define `RunnableParallel` with a dictionary, where each key corresponds to a specific task.\n",
        "\n",
        "```python\n",
        "parallel_runnable = RunnableParallel({\n",
        "    \"api_result\": api_runnable,\n",
        "    \"processed_result\": process_runnable,\n",
        "})\n",
        "\n",
        "result = parallel_runnable.invoke(\"example query\")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```python\n",
        "{\n",
        "    \"api_result\": \"Data from API for example query\",\n",
        "    \"processed_result\": \"Processed data for example query\"\n",
        "}\n",
        "```\n",
        "\n",
        "Here, the results are returned in a dictionary, allowing you to easily distinguish between the results of different runnables.\n",
        "\n",
        "### Conclusion:\n",
        "`RunnableParallel` is a powerful tool in LangChain for handling concurrent tasks. It allows multiple independent runnables to execute in parallel, reducing the time it takes to complete a workflow, especially in situations where tasks don't depend on each other's results."
      ],
      "metadata": {
        "id": "u0Sbcg5Wn7-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableParallel({'x':RunnablePassthrough(),'y':RunnablePassthrough()})"
      ],
      "metadata": {
        "id": "KHD-KtEHh7mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke('Ayush')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqSj_JjDomj0",
        "outputId": "8a49c5fc-47b5-4674-9503-227174de6d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': 'Ayush', 'y': 'Ayush'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'youtube': '@ayushshauryajha','LinkedIn':'jhaayush2004'}) #passing dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-_9pe-Wopmq",
        "outputId": "cc03c361-6e8b-4213-b13a-93a950dbd842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': {'youtube': '@ayushshauryajha', 'LinkedIn': 'jhaayush2004'},\n",
              " 'y': {'youtube': '@ayushshauryajha', 'LinkedIn': 'jhaayush2004'}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableParallel({'x':RunnablePassthrough(),'Social Account':lambda x : x['LinkedIn']})"
      ],
      "metadata": {
        "id": "EndiBFtWo7ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'youtube': '@ayushshauryajha','LinkedIn':'jhaayush2004'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgOF6pLvp4gI",
        "outputId": "02317b39-6cea-47ed-bc7d-d7f49542f231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': {'youtube': '@ayushshauryajha', 'LinkedIn': 'jhaayush2004'},\n",
              " 'Social Account': 'jhaayush2004'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_website(input: dict):\n",
        "   output = input.get('Website','Not Found')\n",
        "   return output"
      ],
      "metadata": {
        "id": "AMdMSNsRp7Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableParallel({'website':RunnablePassthrough() | RunnableLambda(fetch_website),'Social Account':lambda x : x['LinkedIn']})"
      ],
      "metadata": {
        "id": "TopeFWxqxoGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'youtube': '@ayushshauryajha','LinkedIn':'jhaayush2004'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK6f15qTyMls",
        "outputId": "543cd24b-16e7-4bd7-c6bb-c660e124b79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'website': 'Not Found', 'Social Account': 'jhaayush2004'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'youtube': '@ayushshauryajha','LinkedIn':'jhaayush2004','Website':'www.ayushshauryajha.com'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOaKuEIqypdl",
        "outputId": "8c1d4852-316d-4e17-bc8f-8db2da1b292a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'website': 'www.ayushshauryajha.com', 'Social Account': 'jhaayush2004'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extra_func(input)->str:\n",
        "  return  'I Love Generative AI ♥♥♥'"
      ],
      "metadata": {
        "id": "kw6XL2Svy4MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableParallel({'x': RunnablePassthrough()}).assign(extra=RunnableLambda(extra_func))"
      ],
      "metadata": {
        "id": "du4GMOSczTk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke('hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syXa6jdE1LlR",
        "outputId": "129b2aaa-4b18-4fe2-ba49-40393c63e66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': 'hello', 'extra': 'I Love Generative AI ♥♥♥'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG**"
      ],
      "metadata": {
        "id": "Nhnq-_Me2kbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoRs674o1rVz",
        "outputId": "f95e98f9-cc21-489d-dff0-40145879bb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.2)\n",
            "Collecting starlette<0.41.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.5.15-py3-none-any.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (420 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.0/420.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.40.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=6cd6d9b4c90930340fac5d7e84c756d902a27db99ac7f73519b8439833bc8523\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.15 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.2 httptools-0.6.2 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.7.0 pypika-0.48.9 starlette-0.40.0 uvicorn-0.32.0 uvloop-0.21.0 watchfiles-0.24.0 websockets-13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y65rOvfFVxEn",
        "outputId": "9de2cf84-326d-4678-8ae0-73bc4d529ce6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "U_P7aVFf2omO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# Specify the path to your PDF\n",
        "pdf_path = r\"/content/Resume.pdf\"\n",
        "\n",
        "# Create the loader\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "\n",
        "# Load the PDF and extract its content as documents\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "JsVqOupUVeTI"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RecursiveCharacterTextSplitter` and `CharacterTextSplitter` are both components in LangChain designed to handle the splitting of text documents into manageable chunks. However, they serve slightly different purposes and have different approaches to how they split text. Here's a detailed comparison:\n",
        "\n",
        "### 1. **CharacterTextSplitter**\n",
        "\n",
        "- **Purpose**:\n",
        "  The `CharacterTextSplitter` is a basic text splitter that divides a given text document into smaller chunks based solely on character count. This is useful for scenarios where you want to break text into fixed-size segments without considering the content's structure or meaning.\n",
        "\n",
        "- **How it works**:\n",
        "  It takes an input text and splits it into chunks of a specified maximum character length. If the text exceeds this limit, it breaks the text into separate strings of the defined size.\n",
        "\n",
        "- **Use Case**:\n",
        "  Use `CharacterTextSplitter` when you need a straightforward way to divide text without needing to respect specific boundaries such as sentences or paragraphs.\n",
        "\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "  text = \"This is a sample text that will be split into smaller chunks.\"\n",
        "  splitter = CharacterTextSplitter(chunk_size=10)\n",
        "  chunks = splitter.split_text(text)\n",
        "  print(chunks)  # Output: ['This is a ', 'sample tex', 't that wil', 'l be split', ' into small', 'er chunks.']\n",
        "  ```\n",
        "\n",
        "- **Key Features**:\n",
        "  - Simple and efficient for splitting based on character count.\n",
        "  - Does not consider the content structure (e.g., sentences or paragraphs).\n",
        "\n",
        "### 2. **RecursiveCharacterTextSplitter**\n",
        "\n",
        "- **Purpose**:\n",
        "  The `RecursiveCharacterTextSplitter` is a more advanced splitter that recursively splits text based on specified parameters. It attempts to maintain meaningful text boundaries, such as sentences or paragraphs, while also adhering to a maximum character limit.\n",
        "\n",
        "- **How it works**:\n",
        "  It works by first attempting to split the text into larger chunks (like paragraphs) and then recursively breaking those chunks down further if they exceed the specified character limit. This allows it to create more coherent and contextually relevant segments of text.\n",
        "\n",
        "- **Use Case**:\n",
        "  Use `RecursiveCharacterTextSplitter` when you want to maintain the structure of the text and ensure that chunks are meaningful. This is especially important in applications like natural language processing or when using language models that benefit from context.\n",
        "\n",
        "- **Example**:\n",
        "  ```python\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "  text = \"This is a sample text. It contains multiple sentences. This text will be split into meaningful chunks.\"\n",
        "  splitter = RecursiveCharacterTextSplitter(chunk_size=30, chunk_overlap=5)\n",
        "  chunks = splitter.split_text(text)\n",
        "  print(chunks)  # Output: ['This is a sample text. It', 'contains multiple sentences. This', 'text will be split into meaningful chunks.']\n",
        "  ```\n",
        "\n",
        "- **Key Features**:\n",
        "  - Maintains text structure and coherence by respecting sentence and paragraph boundaries.\n",
        "  - Allows for chunk overlap, which can be useful in scenarios where context from adjacent chunks is necessary.\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Feature                           | CharacterTextSplitter                        | RecursiveCharacterTextSplitter                     |\n",
        "|-----------------------------------|----------------------------------------------|---------------------------------------------------|\n",
        "| **Purpose**                       | Simple splitting by character count          | Recursive splitting while maintaining structure    |\n",
        "| **Splitting Method**              | Fixed-size chunks based on character limit   | Splits based on larger segments (like paragraphs) and then further breaks them down |\n",
        "| **Context Preservation**           | Does not preserve context or structure       | Preserves context and structure in chunks         |\n",
        "| **Chunk Overlap Support**         | No overlap; chunks are distinct              | Supports overlap between chunks                    |\n",
        "| **Use Case**                      | Suitable for basic text splitting            | Ideal for NLP tasks requiring context              |\n",
        "\n",
        "### Summary:\n",
        "- **`CharacterTextSplitter`** is useful for straightforward text division based on character count, making it efficient but potentially losing context.\n",
        "- **`RecursiveCharacterTextSplitter`** provides a more sophisticated method of splitting text, retaining contextual meaning and structure, making it more suitable for natural language processing tasks where context matters.\n",
        "\n",
        "Choose between the two based on your specific needs—whether you prioritize simplicity and speed (`CharacterTextSplitter`) or context and meaningful chunks (`RecursiveCharacterTextSplitter`)."
      ],
      "metadata": {
        "id": "pz4zopYTWmOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating Chunks using RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=50,\n",
        "    chunk_overlap=10,\n",
        "    length_function=len\n",
        ")"
      ],
      "metadata": {
        "id": "q9IqLpS0V5Wk"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_docs = text_splitter.split_documents(documents=docs)"
      ],
      "metadata": {
        "id": "q2LHTJQLWDLh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `HuggingFaceBgeEmbeddings` and `HuggingFaceEmbeddings` are both classes in LangChain used to generate embeddings for text, but they differ in terms of the models and embedding generation techniques they use. Here’s a comparison of the two:\n",
        "\n",
        "### 1. **`HuggingFaceEmbeddings`**\n",
        "   - **General Overview**:\n",
        "     `HuggingFaceEmbeddings` is a more general class used to generate embeddings from any model available on the Hugging Face Hub that supports text embeddings (e.g., BERT, GPT, RoBERTa, etc.).\n",
        "   - **How it works**:\n",
        "     - It can be configured to use any transformer model from Hugging Face that generates embeddings for NLP tasks.\n",
        "     - Embeddings generated by these models are typically context-aware and capture the semantic meaning of sentences or words based on the model’s pre-training.\n",
        "   - **Applications**:\n",
        "     Commonly used for tasks like semantic search, text classification, clustering, or any task requiring deep representations of text.\n",
        "   - **Model Flexibility**:\n",
        "     It allows you to specify any Hugging Face model, including general-purpose transformers, to generate embeddings.\n",
        "   \n",
        "   #### Example Usage:\n",
        "   ```python\n",
        "   from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "   # Initialize with a Hugging Face model, such as BERT or RoBERTa\n",
        "   embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "   # Generate embeddings for a sample text\n",
        "   vector = embeddings.embed_query(\"Sample text\")\n",
        "   ```\n",
        "\n",
        "### 2. **`HuggingFaceBgeEmbeddings`**\n",
        "   - **General Overview**:\n",
        "     `HuggingFaceBgeEmbeddings` specifically leverages the **BGE (Base General Embeddings)** model family, which is optimized for generating embeddings for tasks like **retrieval-augmented generation (RAG)**, dense retrieval, and other retrieval tasks.\n",
        "   - **How it works**:\n",
        "     - It uses BGE models, which are designed to produce embeddings optimized for retrieval and ranking tasks. The embeddings are focused on capturing information useful for similarity searches.\n",
        "     - These embeddings are particularly suited for use cases where the goal is to match or rank text snippets, such as in question-answer retrieval or document similarity.\n",
        "   - **Applications**:\n",
        "     Primarily used in retrieval tasks, such as document retrieval, semantic search, or question answering pipelines where the model retrieves relevant documents or passages.\n",
        "   - **Model Specificity**:\n",
        "     It is pre-configured to use BGE models, which are tuned specifically for effective retrieval tasks.\n",
        "\n",
        "   #### Example Usage:\n",
        "   ```python\n",
        "   from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "   # Initialize with a BGE model, typically optimized for retrieval tasks\n",
        "   embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "   # Generate embeddings for a query or document\n",
        "   vector = embeddings.embed_query(\"Sample query text\")\n",
        "   ```\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| **Feature**                      | **HuggingFaceEmbeddings**                                  | **HuggingFaceBgeEmbeddings**                               |\n",
        "|-----------------------------------|------------------------------------------------------------|------------------------------------------------------------|\n",
        "| **Model Flexibility**             | Can use any Hugging Face model (BERT, RoBERTa, etc.)        | Specifically optimized for **BGE models**                   |\n",
        "| **Target Use Case**               | General-purpose embeddings for a variety of tasks           | Primarily for **retrieval** and **ranking** tasks            |\n",
        "| **Model Focus**                   | Models like BERT, GPT, RoBERTa, etc.                        | Models optimized for **similarity search** and **retrieval** |\n",
        "| **Application**                   | Wide range of NLP tasks (classification, clustering, etc.)  | Dense retrieval, RAG, question-answer retrieval              |\n",
        "| **Embedding Optimization**        | General-purpose semantic embeddings                         | Retrieval-specific embeddings                               |\n",
        "\n",
        "### Summary:\n",
        "- **`HuggingFaceEmbeddings`** is more flexible and can use any transformer model from Hugging Face, making it suitable for a wide range of tasks.\n",
        "- **`HuggingFaceBgeEmbeddings`** is tailored for retrieval tasks, with embeddings optimized for dense retrieval and ranking tasks, making it a better choice for use cases like search and retrieval-augmented generation."
      ],
      "metadata": {
        "id": "LxdfEuPFXYVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###  BGE Embddings\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs,\n",
        ")\n"
      ],
      "metadata": {
        "id": "g031jzerWHOe"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating Retriever using Vector DB\n",
        "\n",
        "db = Chroma.from_documents(new_docs, embeddings)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "7wM87ARrWMgo"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **PromptTemplate** is ideal for generating prompts where a single input is passed to a model, often in one-off queries or tasks.\n",
        "\n",
        "\n",
        "- **ChatPromptTemplate** is designed for building multi-turn conversations with chat-based models, where the interaction has multiple roles and turns."
      ],
      "metadata": {
        "id": "QMqwvgnjXAal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "DOBwyGPOWPq0"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    )"
      ],
      "metadata": {
        "id": "5_LqZm97WR7-"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"At what institute is Ayush Currently studying ?\""
      ],
      "metadata": {
        "id": "GDiLxshRWVZS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "O3BKMEMtWXOo",
        "outputId": "92d08e1b-0cb8-4542-af03-19f7e6ce4bb6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ayush is currently studying at **IIIT Ranchi**. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "result = retrieval_chain.invoke(question)\n",
        "\n",
        "print('Time taken:',time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZuQLH7CWfXT",
        "outputId": "60049f00-5587-4afc-9d4e-53e3e2529899"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 1.0823438167572021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ainvoke() {asynchronous --> a}\n",
        "In LangChain, `ainvoke()` is a method used to invoke runnables asynchronously. It can be faster in certain cases due to the nature of asynchronous execution, which allows for non-blocking, parallel processing. Here’s how and why `ainvoke()` can be faster:\n",
        "\n",
        "### 1. **Parallel Processing of I/O-bound Tasks**:\n",
        "   Asynchronous execution is particularly beneficial when dealing with tasks that are **I/O-bound** (i.e., tasks that involve waiting for external resources such as APIs, databases, or file systems). For example, when sending requests to external APIs like language models or web scraping, most of the time is spent waiting for the response. By using `ainvoke()`, multiple requests can be made concurrently, rather than waiting for one request to complete before starting the next.\n",
        "   \n",
        "   - **Without `ainvoke()`** (sequential execution):\n",
        "     - Each request waits for a response before making the next one, which results in a total time equal to the sum of all response times.\n",
        "   \n",
        "   - **With `ainvoke()`** (asynchronous execution):\n",
        "     - Multiple requests can be sent out in parallel, and responses are processed as soon as they are available, reducing the total waiting time.\n",
        "\n",
        "   ### Example:\n",
        "   ```python\n",
        "   async def fetch_responses():\n",
        "       result_1 = await chain1.ainvoke(input_data)\n",
        "       result_2 = await chain2.ainvoke(input_data)\n",
        "       return result_1, result_2\n",
        "   ```\n",
        "   Here, `ainvoke()` allows both `chain1` and `chain2` to process concurrently, reducing overall runtime, especially if each chain involves a time-consuming I/O task.\n",
        "\n",
        "### 2. **Efficient Utilization of Resources**:\n",
        "   When you invoke runnables sequentially (using `invoke()`), each runnable waits for the previous one to finish, leading to idle time. Asynchronous execution (`ainvoke()`) helps maximize the utilization of resources (like CPU or network bandwidth) by working on multiple tasks simultaneously, without blocking the execution flow.\n",
        "\n",
        "   This is particularly useful when:\n",
        "   - Running multiple independent tasks that don’t need to wait for each other.\n",
        "   - Working with models that perform independent computations or calls to external services.\n",
        "   \n",
        "### 3. **Concurrency Benefits in Distributed Systems**:\n",
        "   When working with APIs (e.g., Hugging Face, OpenAI), distributed systems, or cloud-based services, using `ainvoke()` takes advantage of their ability to handle concurrent requests. Rather than waiting for each API call to complete, asynchronous execution sends out multiple requests simultaneously, dramatically improving throughput and reducing total execution time.\n",
        "\n",
        "   ### Example:\n",
        "   If you are processing multiple independent chunks of text using a large language model, calling `ainvoke()` on multiple requests allows them to be processed concurrently:\n",
        "   ```python\n",
        "   async def fetch_embedding(text):\n",
        "       return await embedding_chain.ainvoke(text)\n",
        "\n",
        "   async def fetch_all_embeddings(texts):\n",
        "       results = await asyncio.gather(*[fetch_embedding(text) for text in texts])\n",
        "       return results\n",
        "   ```\n",
        "\n",
        "### 4. **Reduced Latency in Sequential Processes**:\n",
        "   Even when tasks are dependent (e.g., where the result of one is required for the next), `ainvoke()` can help by overlapping I/O with computation. For example, if part of a task is waiting for an API response, `ainvoke()` allows the program to perform other computations while waiting for the response.\n",
        "\n",
        "### When `ainvoke()` is Faster:\n",
        "- **When tasks are independent**: If you have multiple independent tasks (like making API calls, processing chunks of data, or parallel computations), `ainvoke()` can execute them concurrently.\n",
        "- **When tasks are I/O-bound**: If your workflow involves waiting for I/O tasks (like API responses, database queries, file read/write operations), `ainvoke()` allows you to handle these tasks without blocking the flow of execution.\n",
        "- **Parallel requests to external services**: When interacting with external services (like language models or databases), `ainvoke()` allows you to send multiple requests in parallel and handle responses as they arrive, significantly reducing latency.\n",
        "\n",
        "### Summary:\n",
        "`ainvoke()` can be faster because it allows for **concurrent execution** of tasks, especially useful for **I/O-bound tasks** or **independent parallel operations**. By invoking tasks asynchronously, you make more efficient use of resources and reduce overall execution time by running multiple tasks in parallel. This is in contrast to blocking, sequential execution where one task has to fully complete before the next begins."
      ],
      "metadata": {
        "id": "scC5VDbMXePQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "result = retrieval_chain.ainvoke(question)\n",
        "\n",
        "print('Time taken:',time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVrQ15SgWg5-",
        "outputId": "e2903c6a-a1cc-4faf-90e4-fb1c2494e5ad"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.0006890296936035156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "batch_output = retrieval_chain.batch([\n",
        "                        \"what is institute of Ayush in which he's studying?\",\n",
        "                        \"can you highlight 3 main technology stack of AYUSH?\"\n",
        "                       ])\n",
        "\n",
        "print('Time taken:',time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCii9Hr-X6rm",
        "outputId": "19d1b73b-2e67-468a-efc6-45d3d7046e86"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 1.6011250019073486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhXDvClEYOZ7",
        "outputId": "5e4b9b98-ce10-4814-fdb9-8616202f42cc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The provided text mentions \"Indian Institute of Information Technology\" but doesn\\'t explicitly state that\\'s where Ayush is studying. \\n', 'Based on the provided text, the main technology stack used by Ayush seems to be:\\n\\n1. **Tensorflow**\\n2. **Python** \\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "batch_output = await retrieval_chain.abatch([\n",
        "                        \"what is institute of Ayush in which he's studying?\",\n",
        "                        \"can you highlight 3 main technology stack of AYUSH?\"\n",
        "                       ])\n",
        "\n",
        "print('Time taken:',time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYX4bGagYSev",
        "outputId": "d6045734-26e7-485c-b1ba-04b42b2cebca"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 3.7284388542175293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Daa7K2ZYeVf",
        "outputId": "23067959-2675-42b2-defb-59facb2d1679"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Based on the provided text, we can\\'t determine where Ayush is studying. We only know the name \"Ayush Shaurya Jha\" and a repeated phrase \"Indian Institute of Information Technology\". \\n', \"Based on the provided text, the **3 main technology stacks** used by Ayush are:\\n\\n1. **B.Tech**: This indicates Ayush's educational background, suggesting a strong foundation in engineering principles and technical skills.\\n2. **Tensorflow**: This highlights Ayush's experience with a popular open-source machine learning platform, showcasing his ability to work with complex algorithms and data.\\n3. **Python**: This demonstrates Ayush's proficiency in a versatile programming language widely used in data science and machine learning applications. \\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Itemgetter\n",
        "`itemgetter` is a function provided by the `operator` module in Python that allows you to retrieve specific items (elements) from a collection, such as lists, tuples, or dictionaries, by index or key. It is especially useful for sorting or extracting specific fields from data structures in a concise and efficient way.\n",
        "\n",
        "### Common Use Cases for `itemgetter`:\n",
        "- Sorting a list of tuples based on specific elements.\n",
        "- Retrieving multiple elements from a sequence (list, tuple).\n",
        "- Accessing values from a dictionary by key.\n",
        "\n",
        "### Syntax:\n",
        "```python\n",
        "from operator import itemgetter\n",
        "\n",
        "itemgetter(index_or_key, ...)\n",
        "```\n",
        "\n",
        "### Parameters:\n",
        "- **index_or_key**: The index (for sequences like lists/tuples) or key (for dictionaries) of the element(s) to retrieve. Multiple indices/keys can be passed to extract multiple items at once.\n",
        "\n",
        "### Examples:\n",
        "\n",
        "#### 1. Sorting a List of Tuples by a Specific Field:\n",
        "If you have a list of tuples and want to sort the list by a specific field, `itemgetter` can be used to specify the field for sorting.\n",
        "\n",
        "```python\n",
        "from operator import itemgetter\n",
        "\n",
        "data = [('apple', 3), ('banana', 1), ('cherry', 2)]\n",
        "\n",
        "# Sort by the second element (index 1) of each tuple\n",
        "sorted_data = sorted(data, key=itemgetter(1))\n",
        "print(sorted_data)\n",
        "# Output: [('banana', 1), ('cherry', 2), ('apple', 3)]\n",
        "```\n",
        "\n",
        "#### 2. Accessing Multiple Elements from a Tuple/List:\n",
        "You can retrieve multiple items at once by passing multiple indices to `itemgetter`.\n",
        "\n",
        "```python\n",
        "from operator import itemgetter\n",
        "\n",
        "data = (10, 20, 30, 40)\n",
        "\n",
        "# Retrieve the first and third elements (indices 0 and 2)\n",
        "result = itemgetter(0, 2)(data)\n",
        "print(result)  # Output: (10, 30)\n",
        "```\n",
        "\n",
        "#### 3. Sorting a List of Dictionaries by a Specific Key:\n",
        "When working with a list of dictionaries, `itemgetter` can extract a specific value by key and use it for sorting.\n",
        "\n",
        "```python\n",
        "from operator import itemgetter\n",
        "\n",
        "data = [{'name': 'John', 'age': 25}, {'name': 'Alice', 'age': 22}, {'name': 'Bob', 'age': 30}]\n",
        "\n",
        "# Sort by the 'age' key in each dictionary\n",
        "sorted_data = sorted(data, key=itemgetter('age'))\n",
        "print(sorted_data)\n",
        "# Output: [{'name': 'Alice', 'age': 22}, {'name': 'John', 'age': 25}, {'name': 'Bob', 'age': 30}]\n",
        "```\n",
        "\n",
        "#### 4. Using `itemgetter` in `max()` or `min()`:\n",
        "`itemgetter` can also be used with functions like `max()` or `min()` to find the element with the highest or lowest value based on a specific field.\n",
        "\n",
        "```python\n",
        "from operator import itemgetter\n",
        "\n",
        "data = [('apple', 3), ('banana', 1), ('cherry', 2)]\n",
        "\n",
        "# Get the tuple with the maximum value in the second element\n",
        "max_item = max(data, key=itemgetter(1))\n",
        "print(max_item)\n",
        "# Output: ('apple', 3)\n",
        "```\n",
        "\n",
        "### Advantages of `itemgetter`:\n",
        "- **Concise and Readable**: It simplifies the process of retrieving elements from complex data structures.\n",
        "- **Efficient**: `itemgetter` is implemented in C, making it faster than equivalent custom lambda functions.\n",
        "- **Versatile**: Works on any data structure that supports indexing (lists, tuples) or key access (dictionaries).\n",
        "\n",
        "### Summary:\n",
        "`itemgetter` is a powerful tool for retrieving and working with elements in collections based on their index or key. It's most commonly used for sorting or selecting specific fields from tuples or dictionaries, making your code more concise and efficient."
      ],
      "metadata": {
        "id": "rP4qULnVeGSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict1 = {'youtube': '@ayushshauryajha','LinkedIn':'jhaayush2004', 'website' : 'www.shaurysphinx.com'}\n",
        "my_dict1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghlLyBlPYnq0",
        "outputId": "69cc6ac0-8608-4e16-ac79-5352b892212a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'youtube': '@ayushshauryajha',\n",
              " 'LinkedIn': 'jhaayush2004',\n",
              " 'website': 'www.shaurysphinx.com'}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "Webpage = itemgetter('website')"
      ],
      "metadata": {
        "id": "_bwdJnBvZk_m"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Webpage(my_dict1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "--TzPzEFaAiP",
        "outputId": "f7d01254-5656-4c49-da71-86df1deefab0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'www.shaurysphinx.com'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "YfFJ1nZFaC5g"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain = (\n",
        "    RunnableParallel({\"context\": itemgetter('question') | retriever,\n",
        "                       \"question\": itemgetter('question'),\n",
        "                       \"language\": itemgetter('language')\n",
        "                       })\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    )"
      ],
      "metadata": {
        "id": "fEbk569Vekmz"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### itemgetter only works with dictionaries , input has to be a dict\n",
        "\n",
        "response = retrieval_chain.invoke({'question': \"Who is Ayush?\",\n",
        "                        'language': \"Russian\"})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxTFiHvae1g9",
        "outputId": "334e66f4-7efc-46ec-857c-24ad34f969cd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Судя по предоставленному контексту, Аюш Шаурья Джа - это человек с адресом электронной почты ayush.2022ug3013@iiitranchi.ac.in и номером телефона +91-7903998389.  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stream**\n",
        "In LangChain, `chain.stream` refers to the ability to execute a chain of actions (or a sequence of runnables) while streaming the output incrementally as it becomes available. This is especially useful when dealing with large or long-running tasks, where you want to receive partial results as soon as they are produced, instead of waiting for the entire process to complete.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "1. **Chaining**:\n",
        "   - A \"chain\" in LangChain is essentially a sequence of actions or tasks (often using `runnables` like `LLMs`, transformations, or other steps).\n",
        "   - Each step of the chain can pass its output to the next step in the sequence.\n",
        "\n",
        "2. **Streaming**:\n",
        "   - Instead of waiting for the entire task to complete before getting the results, you can stream the intermediate results. This is useful in cases like long LLM generations, fetching large datasets, or API calls where the response takes time.\n",
        "\n",
        "3. **Use Case**:\n",
        "   - For example, if you're using an LLM (Language Model) for a task and want to see the output as it's being generated (as in a chat or question-answering system), streaming allows you to get words or sentences as they are produced, not at the end of the full response.\n",
        "   \n",
        "### How `chain.stream` Works:\n",
        "- `chain.stream` initiates the chain's execution and returns results as they are available.\n",
        "- Each component (or runnable) in the chain that supports streaming will output data incrementally, and you can capture that stream in real-time.\n",
        "- Streaming is commonly used with chains that involve LLMs (like ChatGPT-style models) or other time-intensive operations where it’s valuable to see partial outputs.\n",
        "\n",
        "### Example Usage:\n",
        "\n",
        "```python\n",
        "# Assuming you have a LangChain setup\n",
        "# Example of running a chain with stream method\n",
        "\n",
        "async for output in chain.stream(input_data):\n",
        "    print(output)\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- `chain` is the LangChain object or runnable you're executing.\n",
        "- `input_data` is the input you're providing to the chain.\n",
        "- The chain's `.stream()` method produces incremental results, which you can process one by one.\n",
        "\n",
        "### Benefits of `chain.stream`:\n",
        "1. **Efficiency**: For large outputs, streaming can make the system more responsive because you don’t have to wait for the entire output to be generated before seeing results.\n",
        "2. **User Experience**: In a UI (like a chatbot), the user can see partial results, which improves the perception of speed.\n",
        "3. **Real-time Processing**: You can begin processing or displaying data as soon as it is available.\n",
        "\n",
        "### When to Use:\n",
        "- **LLM Responses**: Stream responses from a large language model as it's generating a completion.\n",
        "- **Long-running tasks**: Stream intermediate results of long-running processes or tasks that have multiple stages.\n"
      ],
      "metadata": {
        "id": "616zPiDbfbUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "GBM6_lLAe9su"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in chain.stream({'skill':'Big Data'}):\n",
        "    print(s.content,end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsSVySVPfRMh",
        "outputId": "81817cd0-aee6-45ac-b53d-f0e91ef362db"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's great! Big Data is a fascinating and ever-evolving field. To get you started on the right foot, here are five key areas to focus on:\n",
            "\n",
            "**1. Programming Fundamentals:**\n",
            "\n",
            "* **Why:** You'll need to work with massive datasets, and programming languages are essential for manipulating and analyzing them.\n",
            "* **What to Learn:**\n",
            "    * **Python:** Widely used in data science and Big Data due to its extensive libraries (like Pandas, NumPy, and Scikit-learn) and relatively easy syntax.\n",
            "    * **Java:**  A robust and scalable language commonly used in Hadoop and other Big Data frameworks.\n",
            "    * **Scala:** A concise and powerful language that runs on the Java Virtual Machine (JVM) and is often preferred for Spark.\n",
            "\n",
            "**2. Distributed Computing:**\n",
            "\n",
            "* **Why:** Big Data often exceeds the capacity of a single machine, so you need to understand how to process it across clusters of computers.\n",
            "* **What to Learn:**\n",
            "    * **Hadoop:** An open-source framework for distributed storage (HDFS) and processing (MapReduce) of large datasets.\n",
            "    * **Apache Spark:** A fast and general-purpose engine for large-scale data processing, often used for real-time analytics and machine learning.\n",
            "\n",
            "**3. Databases and Data Warehousing:**\n",
            "\n",
            "* **Why:** You need to know where and how to store your Big Data efficiently, as well as how to retrieve it for analysis.\n",
            "* **What to Learn:**\n",
            "    * **NoSQL Databases:**  Like MongoDB or Cassandra, handle unstructured or semi-structured data better than traditional relational databases.\n",
            "    * **Data Warehousing Concepts:** Learn about ETL (Extract, Transform, Load) processes, data modeling for analytics, and OLAP (Online Analytical Processing) cubes.\n",
            "\n",
            "**4. Data Visualization and Communication:**\n",
            "\n",
            "* **Why:**  It's not enough to just analyze data; you need to be able to present your findings clearly and compellingly to stakeholders.\n",
            "* **What to Learn:**\n",
            "    * **Data Visualization Tools:** Tableau, Power BI, or Python libraries like Matplotlib and Seaborn are excellent for creating insightful charts and dashboards.\n",
            "    * **Storytelling with Data:** Develop your ability to communicate insights effectively through visual representations and narratives.\n",
            "\n",
            "**5. Cloud Computing Platforms:**\n",
            "\n",
            "* **Why:** Cloud platforms offer scalable and cost-effective solutions for storing, processing, and analyzing Big Data.\n",
            "* **What to Learn:**\n",
            "    * **AWS:** Amazon Web Services provides a wide range of Big Data services like EMR (Elastic MapReduce), Redshift, and Athena.\n",
            "    * **Azure:** Microsoft Azure offers HDInsight (Hadoop), Azure Databricks (Spark), and Cosmos DB for Big Data needs.\n",
            "    * **GCP:** Google Cloud Platform provides services like BigQuery, Cloud Dataflow, and Cloud Storage.\n",
            "\n",
            "**Important Note:** Big Data is a vast field. Start with the fundamentals (programming and distributed computing), then gradually explore more specialized areas like data warehousing, machine learning on Big Data, or stream processing based on your interests and career goals. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tools**\n",
        "Great question! You can use either `@tool` or `Tool.from_function()` to register a function as a tool in LangChain, but there are differences in flexibility and use cases.\n",
        "\n",
        "### Differences Between `@tool` and `Tool.from_function()`\n",
        "\n",
        "1. **`@tool` Decorator**:\n",
        "   - This is a convenient and shorthand way to mark a function as a tool.\n",
        "   - It automatically handles the function registration and makes it callable within LangChain workflows.\n",
        "   - Suitable for quick and simple use cases where you don't need to customize much beyond defining the function.\n",
        "\n",
        "   Example:\n",
        "   ```python\n",
        "   from langchain_core.tools import tool\n",
        "\n",
        "   @tool\n",
        "   def multiply(first_number: int, second_number: int):\n",
        "       return first_number * second_number\n",
        "   ```\n",
        "\n",
        "   This is a very simple and convenient way to create tools. The function is automatically converted into a tool when you use the `@tool` decorator.\n",
        "\n",
        "2. **`Tool.from_function()`**:\n",
        "   - Provides more explicit control and flexibility.\n",
        "   - You can provide additional metadata, such as `name` and `description`, and customize the tool creation process.\n",
        "   - It's useful if you want to configure the tool more thoroughly or need to register multiple functions in a more dynamic or programmatic way.\n",
        "\n",
        "   Example:\n",
        "   ```python\n",
        "   from langchain_core.tools import Tool\n",
        "\n",
        "   def multiply(first_number: int, second_number: int):\n",
        "       return first_number * second_number\n",
        "\n",
        "   multiply_tool = Tool.from_function(\n",
        "       func=multiply,\n",
        "       name=\"multiply\",\n",
        "       description=\"Multiplies two numbers together.\"\n",
        "   )\n",
        "   ```\n",
        "\n",
        "### Why Use `Tool.from_function()` Instead of `@tool`?\n",
        "\n",
        "- **Customization**:\n",
        "   With `Tool.from_function()`, you can explicitly set the `name`, `description`, and other metadata that might be useful in different contexts. The decorator doesn't give as much flexibility for custom configurations.\n",
        "   \n",
        "- **Dynamic Registration**:\n",
        "   If you need to programmatically register multiple tools, you might find `Tool.from_function()` easier to work with. For example, if you have many functions or dynamically generated functions, you can loop through and register them using `Tool.from_function()`.\n",
        "\n",
        "- **Clearer for Complex Cases**:\n",
        "   In more complex setups (like when dealing with models that are not OpenAI-based, such as Gemini), using `Tool.from_function()` makes the tool registration process clearer, since you're explicitly managing the function-to-tool conversion.\n",
        "\n",
        "### Why `@tool` Might Be Better for Some Cases:\n",
        "- **Simplicity**:\n",
        "   The `@tool` decorator simplifies the code by letting you turn a function into a tool with just one line. For simple, straightforward use cases, this is all you need.\n",
        "  \n",
        "- **Readability**:\n",
        "   It's often easier to understand at a glance that a particular function is a tool because it's immediately marked with `@tool`.\n",
        "\n",
        "### Conclusion:\n",
        "- Use `@tool` when you want a quick and simple way to register a function as a tool.\n",
        "- Use `Tool.from_function()` when you need more flexibility, customization, or are dynamically creating tools.\n",
        "\n",
        "In the Gemini model case, you can use `@tool` if you're only dealing with basic tools, but `Tool.from_function()` gives you more explicit control, which is often preferable in more advanced scenarios."
      ],
      "metadata": {
        "id": "i8wnbs0Ug2PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Use the @tool decorator to define the multiply tool\n",
        "@tool\n",
        "def multiply(first_number: int, second_number: int):\n",
        "    \"\"\"Multiplies two numbers together.\"\"\"\n",
        "    return first_number * second_number\n",
        "\n",
        "# Bind the tool to the Gemini model (assuming llm is the Gemini model object)\n",
        "model_with_tools = llm.bind(tools=[multiply])\n",
        "\n",
        "# Invoke the model with the question\n",
        "response = model_with_tools.invoke('What is 35 * 46?')\n",
        "\n",
        "# Output the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCotBkBnfTor",
        "outputId": "4fb55c0d-033c-4ce9-f75e-737d4613c368"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"second_number\": 46.0, \"first_number\": 35.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-18c6ac82-8a82-4023-84be-2f02a18d0181-0' tool_calls=[{'name': 'multiply', 'args': {'second_number': 46.0, 'first_number': 35.0}, 'id': '7943e74a-cd62-4fb8-9c23-4e86a087f5c0', 'type': 'tool_call'}] usage_metadata={'input_tokens': 66, 'output_tokens': 24, 'total_tokens': 90}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub5RClxMhJ1l",
        "outputId": "61e6f324-4b9b-40f7-bb04-ca0bd1b72bc4"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"second_number\": 46.0, \"first_number\": 35.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-18c6ac82-8a82-4023-84be-2f02a18d0181-0' tool_calls=[{'name': 'multiply', 'args': {'second_number': 46.0, 'first_number': 35.0}, 'id': '7943e74a-cd62-4fb8-9c23-4e86a087f5c0', 'type': 'tool_call'}] usage_metadata={'input_tokens': 66, 'output_tokens': 24, 'total_tokens': 90}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'tool_call' in response and response['tool_calls']:\n",
        "    # Extract the result from the tool call\n",
        "    tool_call_result = response['tool_calls'][0]  # Get the first tool call (multiply)\n",
        "    print(f\"Tool Call: {tool_call_result}\")\n",
        "\n",
        "    # The tool_call_result will contain the arguments and the computed result\n",
        "    first_number = tool_call_result['args']['first_number']\n",
        "    second_number = tool_call_result['args']['second_number']\n",
        "    result = multiply(first_number, second_number)  # Call multiply directly\n",
        "\n",
        "    print(f\"The result of multiplying {first_number} and {second_number} is: {result}\")\n",
        "else:\n",
        "    print(\"No tool call result found in the response.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t85beGFAhN4L",
        "outputId": "8f189d05-40ee-4790-eaa7-d27f0fbf5ca9"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No tool call result found in the response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2K5vtT5h2yw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}